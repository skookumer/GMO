{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4224202",
   "metadata": {},
   "source": [
    "## DATASET: Market Basket\n",
    "## Topic: Association Rule Mining\n",
    "## Algo: Relim (Recursive Elimination)\n",
    "### Goal: To measure and report \n",
    "- execution time & minimum support threshold\n",
    "- number of frequent itemset\n",
    "- representative associatino rules ( support, confidence, lift)\n",
    "- observations on scalability and memory use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "013ec85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import tracemalloc\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "097075d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 1: Detect unique items and define the global item order by their frequencies\n",
    "df = pd.read_csv('Groceries_dataset.csv')\n",
    "item_counts = Counter(df['itemDescription']) # returns a dictionary: Counter({item: count})\n",
    "unique_by_freq = [item[0] for item in item_counts.most_common()] #item_counts.most_common() returns a list of sorted tuples\n",
    "len(unique_by_freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873371e0",
   "metadata": {},
   "source": [
    "There are 167 unique items in the dataset, and I've sorted them by their frequencies. \n",
    "\n",
    "Next I will group them by member_number and date to form transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "31379e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14963"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions = (df.groupby(['Member_number','Date'])['itemDescription'].apply(list).tolist())\n",
    "item_index = {item: idx for idx, item in enumerate(unique_by_freq)}\n",
    "transactions = [sorted(t, key=lambda x: item_index[x]) for t in transactions] # within each transaction, items are also in global freq order\n",
    "len(transactions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20c0f4e",
   "metadata": {},
   "source": [
    "Together there are 14963 transactions, the original dataset has 38765 rows.\n",
    "\n",
    "For min_support, I've got the item_counts as a dictionary which contain all the unique single item and its count, so I just need to iterate through the dict and get a average support of all the unique items, and I will set that to be my min_support. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e5259e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.005680679008220277)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supports = np.array([count / len(transactions) for count in item_counts.values()])\n",
    "minsup = np.percentile(supports, 50)   # top 50% of single-item supports\n",
    "minsup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abb3439",
   "metadata": {},
   "source": [
    "The min_support I got is 0.0057 which matches the typical min_support range a 10k+ dataset size would have. \n",
    "\n",
    "Then I shall trim my unique single item to eliminate the less frequent items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "9e62c62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_item = [item for item, count in item_counts.items()if count/ len(transactions)>= minsup]\n",
    "\n",
    "freq_item = sorted(freq_item, key= lambda x: item_counts[x], reverse=True)\n",
    "len(freq_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6071c32c",
   "metadata": {},
   "source": [
    "NOW the Relim Algo Begins here, the core of Relim is that:\n",
    "- picking a prefix item from my globally ordered (and trimmed frequent item list)\n",
    "- creating conditional DB of what comes after that item,\n",
    "- recursively mining those smaller DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "a4f59849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global total number of transactions\n",
    "total = len(transactions)\n",
    "\n",
    "def relim(transactions, minsup, prefix=[], results_dict={}, total=len(transactions)):\n",
    "    # Initialize prefix and results container\n",
    "\n",
    "\n",
    "    # Count all items appearing in the current conditional database\n",
    "    counts = Counter(item for t in transactions for item in t)\n",
    "\n",
    "    # Prune infrequent items (compare to global ratio)\n",
    "    # Here c / total uses global total, not local len(condDB)\n",
    "    freq_items = [i for i, c in counts.items() if c / total >= minsup]\n",
    "\n",
    "\n",
    "    # Sort items by ascending frequency for recursion stability (Relim convention)\n",
    "    freq_items.sort(key=lambda x: counts[x])\n",
    "\n",
    "    # Explore each frequent item recursively\n",
    "    for item in freq_items:\n",
    "        new_prefix = prefix + [item]\n",
    "        support = counts[item] / total  # global support ratio\n",
    "        key = frozenset(new_prefix)\n",
    "\n",
    "        # Record only multi-item sets (skip singletons, already known from preprocessing)\n",
    "        # Deduplicate by keeping the highest support if re-encountered\n",
    "        if len(new_prefix) > 1:\n",
    "            if key not in results_dict or support > results_dict[key]:\n",
    "                results_dict[key] = support\n",
    "\n",
    "        # Build conditional database for this item\n",
    "        condDB = []\n",
    "        for t in transactions:\n",
    "            if item in t:\n",
    "                idx = t.index(item)\n",
    "                suffix = t[idx + 1:]  # Only items after current one\n",
    "                if suffix:\n",
    "                    condDB.append(suffix)\n",
    "\n",
    "        # Recurse into the conditional database with same global total\n",
    "        relim(condDB, minsup, new_prefix, results_dict, total)\n",
    "\n",
    "    # Return the dictionary of all discovered itemsets and supports\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "dcb53fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{frozenset({'sausage', 'yogurt'}): 0.005948005079195348,\n",
       " frozenset({'soda', 'yogurt'}): 0.006014836596939116,\n",
       " frozenset({'sausage', 'soda'}): 0.006014836596939116,\n",
       " frozenset({'rolls/buns', 'root vegetables'}): 0.006081668114682885,\n",
       " frozenset({'rolls/buns', 'tropical fruit'}): 0.006081668114682885,\n",
       " frozenset({'rolls/buns', 'yogurt'}): 0.008153445164739691,\n",
       " frozenset({'rolls/buns', 'soda'}): 0.00848760275345853,\n",
       " frozenset({'other vegetables', 'sausage'}): 0.006348994185657956,\n",
       " frozenset({'other vegetables', 'tropical fruit'}): 0.006482657221145492,\n",
       " frozenset({'other vegetables', 'yogurt'}): 0.008353939717970995,\n",
       " frozenset({'other vegetables', 'soda'}): 0.01015839069705273,\n",
       " frozenset({'other vegetables', 'rolls/buns'}): 0.011227694980953017,\n",
       " frozenset({'canned beer', 'whole milk'}): 0.006081668114682885,\n",
       " frozenset({'pastry', 'whole milk'}): 0.0065494887388892606,\n",
       " frozenset({'shopping bags', 'whole milk'}): 0.006616320256633028,\n",
       " frozenset({'pip fruit', 'whole milk'}): 0.006749983292120564,\n",
       " frozenset({'bottled beer', 'whole milk'}): 0.007284635434070708,\n",
       " frozenset({'bottled water', 'whole milk'}): 0.007284635434070708,\n",
       " frozenset({'citrus fruit', 'whole milk'}): 0.007418298469558243,\n",
       " frozenset({'root vegetables', 'whole milk'}): 0.007752456058277083,\n",
       " frozenset({'tropical fruit', 'whole milk'}): 0.008420771235714762,\n",
       " frozenset({'sausage', 'whole milk'}): 0.009222749448639978,\n",
       " frozenset({'whole milk'}): 0.009289580966383746,\n",
       " frozenset({'whole milk', 'yogurt'}): 0.011628684087415625,\n",
       " frozenset({'soda', 'whole milk'}): 0.012430662300340841,\n",
       " frozenset({'rolls/buns', 'whole milk'}): 0.014569270868141415,\n",
       " frozenset({'other vegetables', 'whole milk'}): 0.01537124908106663}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_result = relim(transactions,minsup)\n",
    "freq_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "dbb72c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent itemset found in total: 111 including the singletons\n",
      "27 excluding the singletons\n"
     ]
    }
   ],
   "source": [
    "print(f\"Frequent itemset found in total: {len(freq_result)+len(freq_item)} including the singletons\")\n",
    "print(f\"{len(freq_result)} excluding the singletons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e5bc4a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime:0.0833 sec\n",
      "Peak memory: 216.740 MB\n"
     ]
    }
   ],
   "source": [
    "tracemalloc.start()\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "relim(transactions, minsup)\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "\n",
    "print(f\"Runtime:{end_time - start_time :.4f} sec\")\n",
    "print(f\"Peak memory: {peak/1024.1024 :.3f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f392c91",
   "metadata": {},
   "source": [
    "The most frequent itemset are:\n",
    "\n",
    " frozenset({'whole milk', 'yogurt'}): 0.011628684087415625,\n",
    "\n",
    " frozenset({'soda', 'whole milk'}): 0.012430662300340841,\n",
    "\n",
    " frozenset({'rolls/buns', 'whole milk'}): 0.014569270868141415,\n",
    "\n",
    " frozenset({'other vegetables', 'whole milk'}): 0.01537124908106663,\n",
    " \n",
    " frozenset({'whole milk', 'sausage'}): 0.009222749448639978,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "70fe2682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole milk -> yogurt confidence = sup(whole milk , yogurt)/sup(whole milk)\n",
    "def confidence_and_lift(a,b,itemset_support,total):\n",
    "    # support ratios:\n",
    "    sup_a = item_counts[a]/total\n",
    "    sup_b = item_counts[b]/total\n",
    "\n",
    "\n",
    "   #confidence ratios\n",
    "    conf_a2b = itemset_support / sup_a\n",
    "    conf_b2a = itemset_support / sup_b\n",
    "    #lift - symmetric\n",
    "    lift = itemset_support/ sup_a / sup_b\n",
    "\n",
    "    print(f\"({a} → {b})  | confidence: {conf_a2b:.3f}\")\n",
    "    print(f\"({b} → {a})  | confidence: {conf_b2a:.3f}\")\n",
    "    print(f\"Lift: {lift:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d8731d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(whole milk → sausage)  | confidence: 0.055\n",
      "(sausage → whole milk)  | confidence: 0.149\n",
      "Lift: 0.893\n"
     ]
    }
   ],
   "source": [
    "confidence_and_lift('whole milk','sausage',0.009222749448639978,len(transactions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "6d40daf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(other vegetables → whole milk)  | confidence: 0.121\n",
      "(whole milk → other vegetables)  | confidence: 0.092\n",
      "Lift: 0.725\n"
     ]
    }
   ],
   "source": [
    "confidence_and_lift('other vegetables', 'whole milk', 0.01537124908106663,len(transactions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "432fa03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rolls/buns → whole milk)  | confidence: 0.127\n",
      "(whole milk → rolls/buns)  | confidence: 0.087\n",
      "Lift: 0.760\n"
     ]
    }
   ],
   "source": [
    "confidence_and_lift('rolls/buns', 'whole milk', 0.014569270868141415,len(transactions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "3f8517cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(soda → whole milk)  | confidence: 0.123\n",
      "(whole milk → soda)  | confidence: 0.074\n",
      "Lift: 0.735\n"
     ]
    }
   ],
   "source": [
    "confidence_and_lift('soda', 'whole milk',0.012430662300340841,len(transactions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "17967d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(whole milk → yogurt)  | confidence: 0.070\n",
      "(yogurt → whole milk)  | confidence: 0.130\n",
      "Lift: 0.780\n"
     ]
    }
   ],
   "source": [
    "confidence_and_lift('whole milk', 'yogurt', 0.011628684087415625,len(transactions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS5110",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
